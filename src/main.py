"""
GeliÅŸmiÅŸ Haber Analizi Ana Sistemi

Bu modÃ¼l, geliÅŸmiÅŸ RSS toplayÄ±cÄ± ile entegre edilmiÅŸ
tam haber analizi pipeline'Ä±nÄ± Ã§alÄ±ÅŸtÄ±rÄ±r.
"""

import asyncio
import logging
from datetime import datetime
from typing import Dict, List

# ModÃ¼l importlarÄ±
from hybrid_collector import HybridNewsCollector
from text_processor import TextProcessor
from advanced_analytics import AdvancedAnalytics
from cooccurrence_analyzer import CooccurrenceAnalyzer
from database import NewsDatabase

# Logging ayarlarÄ±
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class EnhancedNewsAnalysis:
    """GeliÅŸmiÅŸ haber analizi sÄ±nÄ±fÄ±"""
    
    def __init__(self):
        """Analiz sistemini baÅŸlat"""
        self.hybrid_collector = HybridNewsCollector()
        self.text_processor = TextProcessor()
        self.advanced_analytics = AdvancedAnalytics()
        self.cooccurrence_analyzer = CooccurrenceAnalyzer()
        self.database = NewsDatabase()
        
    async def run_full_analysis(self) -> Dict:
        """
        Tam haber analizi pipeline'Ä±nÄ± Ã§alÄ±ÅŸtÄ±rÄ±r
        
        Returns:
            Dict: Analiz sonuÃ§larÄ±
        """
        logger.info("ğŸš€ GeliÅŸmiÅŸ Haber Analizi BaÅŸlatÄ±lÄ±yor...")
        logger.info("=" * 60)
        
        try:
            # 1. Hibrit haber toplama (RSS + API)
            logger.info("ğŸš€ Hibrit haber toplama baÅŸlatÄ±lÄ±yor...")
            news_data = await self.hybrid_collector.collect_all_news()
            
            if not news_data:
                logger.error("âŒ HiÃ§ haber toplanamadÄ±!")
                return {}
            
            logger.info(f"âœ… {len(news_data)} haber toplandÄ±")
            
            # 2. Ä°statistikleri hesapla
            stats = self.hybrid_collector.get_statistics(news_data)
            logger.info(f"ğŸ“Š Ä°statistikler: {stats['total_news']} haber (RSS: {stats['rss_news']}, API: {stats['API Haberleri']})")
            
            # 3. Metin iÅŸleme
            logger.info("ğŸ”¤ Metin iÅŸleme baÅŸlatÄ±lÄ±yor...")
            processed_texts = []
            
            for news in news_data:
                # BaÅŸlÄ±k ve Ã¶zeti birleÅŸtir
                full_text = f"{news['title']} {news['summary']}"
                processed_text = self.text_processor.process_text(full_text)
                processed_texts.append(processed_text)
            
            logger.info(f"âœ… {len(processed_texts)} metin iÅŸlendi")
            
            # 4. Temel analiz
            logger.info("ğŸ“Š Temel analiz baÅŸlatÄ±lÄ±yor...")
            basic_analysis = self._perform_basic_analysis(processed_texts, news_data)
            
            # 5. GeliÅŸmiÅŸ analiz
            logger.info("ğŸ” GeliÅŸmiÅŸ analiz baÅŸlatÄ±lÄ±yor...")
            advanced_analysis = self._perform_advanced_analysis(news_data, processed_texts)
            
            # 6. Co-occurrence analizi
            logger.info("ğŸ”— Co-occurrence analizi baÅŸlatÄ±lÄ±yor...")
            cooccurrence_analysis = self._perform_cooccurrence_analysis(processed_texts)
            
            # 7. Metadata oluÅŸtur
            metadata = {
                'total_news': len(news_data),
                'rss_news': stats['rss_news'],
                'API Haberleri': stats['API Haberleri'],
                'sources': list(stats['source_distribution'].keys()),
                'categories': list(stats['category_distribution'].keys()),
                'collection_time': datetime.now().isoformat(),
                'analysis_version': '3.0',
                'collection_type': 'hybrid'
            }
            
            # 8. SonuÃ§larÄ± birleÅŸtir
            analysis_results = {
                'basic_analysis': basic_analysis,
                'advanced_analysis': advanced_analysis,
                'cooccurrence_analysis': cooccurrence_analysis,
                'metadata': metadata
            }
            
            # 9. VeritabanÄ±na kaydet
            logger.info("ğŸ’¾ SonuÃ§lar veritabanÄ±na kaydediliyor...")
            self.database.save_analysis_results(analysis_results)
            
            logger.info("âœ… Analiz tamamlandÄ±!")
            return analysis_results
            
        except Exception as e:
            logger.error(f"âŒ Analiz sÄ±rasÄ±nda hata: {e}")
            import traceback
            traceback.print_exc()
            return {}
    
    def _perform_basic_analysis(self, processed_texts: List[str], news_data: List[Dict]) -> Dict:
        """Temel analiz gerÃ§ekleÅŸtirir"""
        try:
            # TÃ¼m metinleri birleÅŸtir
            combined_text = " ".join(processed_texts)
            
            # Kelime frekansÄ± analizi
            word_frequencies = self.text_processor.get_word_frequencies(combined_text)
            
            # En sÄ±k kullanÄ±lan kelimeler
            top_words = list(word_frequencies.keys())[:20]
            top_frequencies = list(word_frequencies.values())[:20]
            
            # Word cloud verisi
            wordcloud_data = {
                'combined_text': combined_text,
                'word_frequencies': word_frequencies,
                'top_words': top_words,
                'top_frequencies': top_frequencies,
                'total_unique_words': len(word_frequencies)
            }
            
            # Konu modelleme
            topics = self.text_processor.extract_topics(combined_text)
            
            return {
                'word_frequency': {
                    'word_frequencies': [f"('{word}', {freq})" for word, freq in word_frequencies.items()],
                    'total_words': len(combined_text.split()),
                    'unique_words': len(word_frequencies),
                    'avg_word_length': sum(len(word) for word in word_frequencies.keys()) / len(word_frequencies) if word_frequencies else 0,
                    'top_words': top_words,
                    'top_frequencies': top_frequencies
                },
                'wordcloud_data': wordcloud_data,
                'topics': topics
            }
            
        except Exception as e:
            logger.error(f"Temel analiz hatasÄ±: {e}")
            return {}
    
    def _perform_advanced_analysis(self, news_data: List[Dict], processed_texts: List[str]) -> Dict:
        """GeliÅŸmiÅŸ analiz gerÃ§ekleÅŸtirir"""
        try:
            # Kategori analizi
            categories = self.advanced_analytics.analyze_categories(news_data)
            
            # Kaynak analizi
            sources = self.advanced_analytics.analyze_sources(news_data)
            
            # Olay tespiti
            events = self.advanced_analytics.detect_events(news_data)
            
            # Benzerlik analizi
            similarities = self.advanced_analytics.find_similar_news(news_data)
            
            return {
                'categories': categories,
                'sources': sources,
                'events': events,
                'similarities': similarities
            }
            
        except Exception as e:
            logger.error(f"GeliÅŸmiÅŸ analiz hatasÄ±: {e}")
            return {}
    
    def _perform_cooccurrence_analysis(self, processed_texts: List[str]) -> Dict:
        """Co-occurrence analizi gerÃ§ekleÅŸtirir"""
        try:
            # Co-occurrence analizi
            cooccurrences = self.cooccurrence_analyzer.analyze_cooccurrences(processed_texts)
            
            # AÄŸ analizi
            network_data = self.cooccurrence_analyzer.create_network_data(cooccurrences)
            
            return {
                'cooccurrences': cooccurrences,
                'network_data': network_data
            }
            
        except Exception as e:
            logger.error(f"Co-occurrence analiz hatasÄ±: {e}")
            return {}

# Ana Ã§alÄ±ÅŸtÄ±rma fonksiyonu
async def main():
    """Ana analiz sistemini Ã§alÄ±ÅŸtÄ±r"""
    analyzer = EnhancedNewsAnalysis()
    
    print("ğŸš€ GeliÅŸmiÅŸ Haber Analizi Sistemi")
    print("=" * 60)
    
    # Tam analizi Ã§alÄ±ÅŸtÄ±r
    results = await analyzer.run_full_analysis()
    
    if results:
        print("\nâœ… ANALÄ°Z TAMAMLANDI!")
        print(f"ğŸ“Š Toplam haber: {results['metadata']['total_news']}")
        print(f"ğŸ“¡ Kaynaklar: {', '.join(results['metadata']['sources'])}")
        print(f"ğŸ·ï¸ Kategoriler: {', '.join(results['metadata']['categories'])}")
        print(f"â° Analiz zamanÄ±: {results['metadata']['collection_time']}")
        
        # Temel analiz sonuÃ§larÄ±
        if 'basic_analysis' in results and 'word_frequency' in results['basic_analysis']:
            word_freq = results['basic_analysis']['word_frequency']
            print(f"\nğŸ“ˆ TEMEL ANALÄ°Z SONUÃ‡LARI:")
            print(f"  Toplam kelime: {word_freq['total_words']}")
            print(f"  Benzersiz kelime: {word_freq['unique_words']}")
            print(f"  Ortalama kelime uzunluÄŸu: {word_freq['avg_word_length']:.2f}")
            
            print(f"\nğŸ”¤ EN SIK KULLANILAN KELÄ°MELER:")
            for i, (word, freq) in enumerate(zip(word_freq['top_words'][:10], word_freq['top_frequencies'][:10]), 1):
                print(f"  {i}. {word}: {freq}")
        
        # GeliÅŸmiÅŸ analiz sonuÃ§larÄ±
        if 'advanced_analysis' in results and 'categories' in results['advanced_analysis']:
            categories = results['advanced_analysis']['categories']
            if 'category_distribution' in categories:
                print(f"\nğŸ·ï¸ KATEGORÄ° DAÄILIMI:")
                for category, count in categories['category_distribution'].items():
                    print(f"  {category}: {count} haber")
        
        print(f"\nğŸ’¾ SonuÃ§lar veritabanÄ±na kaydedildi!")
        print(f"ğŸŒ Dashboard'u gÃ¶rÃ¼ntÃ¼lemek iÃ§in: streamlit run dashboard_fixed.py")
        
    else:
        print("\nâŒ Analiz baÅŸarÄ±sÄ±z oldu!")

if __name__ == "__main__":
    asyncio.run(main()) 