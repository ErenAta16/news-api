"""
Network Analizi ModÃ¼lÃ¼

Bu modÃ¼l, haber metinlerindeki kelimeler arasÄ±ndaki iliÅŸkileri
network grafikleri olarak gÃ¶rselleÅŸtirir ve analiz eder.
"""

import networkx as nx
import matplotlib.pyplot as plt
import numpy as np
from collections import Counter, defaultdict
from typing import List, Dict, Tuple, Set
import pandas as pd
from itertools import combinations
import logging

# Logging ayarlarÄ±
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class NetworkAnalyzer:
    """Haber metinlerindeki kelime iliÅŸkilerini analiz eden sÄ±nÄ±f"""
    
    def __init__(self, min_cooccurrence: int = 2, max_nodes: int = 50):
        """
        Network analiz ediciyi baÅŸlat
        
        Args:
            min_cooccurrence (int): Minimum birlikte geÃ§me sayÄ±sÄ±
            max_nodes (int): Maksimum node sayÄ±sÄ±
        """
        self.min_cooccurrence = min_cooccurrence
        self.max_nodes = max_nodes
        self.graph = None
        
    def extract_cooccurrences(self, texts: List[str]) -> Dict[Tuple[str, str], int]:
        """
        Metinlerden birlikte geÃ§en kelime Ã§iftlerini Ã§Ä±karÄ±r
        
        Args:
            texts (List[str]): TemizlenmiÅŸ metinlerin listesi
            
        Returns:
            Dict[Tuple[str, str], int]: Kelime Ã§iftleri ve birlikte geÃ§me sayÄ±larÄ±
        """
        logger.info("Birlikte geÃ§en kelimeler Ã§Ä±karÄ±lÄ±yor...")
        
        cooccurrences = defaultdict(int)
        
        for text in texts:
            words = text.split()
            if len(words) < 2:
                continue
                
            # Kelime Ã§iftlerini oluÅŸtur
            for i in range(len(words)):
                for j in range(i + 1, min(i + 3, len(words))):  # 2-3 kelime aralÄ±ÄŸÄ±nda
                    word1, word2 = sorted([words[i], words[j]])
                    if word1 != word2:
                        cooccurrences[(word1, word2)] += 1
        
        # Minimum eÅŸiÄŸi geÃ§en Ã§iftleri filtrele
        filtered_cooccurrences = {
            pair: count for pair, count in cooccurrences.items() 
            if count >= self.min_cooccurrence
        }
        
        logger.info(f"{len(filtered_cooccurrences)} kelime Ã§ifti bulundu")
        return filtered_cooccurrences
    
    def build_network(self, cooccurrences: Dict[Tuple[str, str], int]) -> nx.Graph:
        """
        Network grafiÄŸini oluÅŸturur
        
        Args:
            cooccurrences (Dict[Tuple[str, str], int]): Kelime Ã§iftleri
            
        Returns:
            nx.Graph: Network grafiÄŸi
        """
        logger.info("Network grafiÄŸi oluÅŸturuluyor...")
        
        # GrafiÄŸi oluÅŸtur
        G = nx.Graph()
        
        # En sÄ±k geÃ§en kelimeleri bul
        word_counts = defaultdict(int)
        for (word1, word2), count in cooccurrences.items():
            word_counts[word1] += count
            word_counts[word2] += count
        
        # En popÃ¼ler kelimeleri seÃ§
        top_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:self.max_nodes]
        top_word_set = set(word for word, _ in top_words)
        
        # Node'larÄ± ekle
        for word, count in top_words:
            G.add_node(word, weight=count, size=min(count * 2, 50))
        
        # Edge'leri ekle
        for (word1, word2), count in cooccurrences.items():
            if word1 in top_word_set and word2 in top_word_set:
                G.add_edge(word1, word2, weight=count, width=min(count, 10))
        
        self.graph = G
        logger.info(f"Network oluÅŸturuldu: {G.number_of_nodes()} node, {G.number_of_edges()} edge")
        return G
    
    def calculate_network_metrics(self, G: nx.Graph) -> Dict:
        """
        Network metriklerini hesaplar
        
        Args:
            G (nx.Graph): Network grafiÄŸi
            
        Returns:
            Dict: Network metrikleri
        """
        metrics = {}
        
        # Temel metrikler
        metrics['num_nodes'] = G.number_of_nodes()
        metrics['num_edges'] = G.number_of_edges()
        metrics['density'] = nx.density(G)
        
        # Merkezilik metrikleri
        if G.number_of_nodes() > 1:
            metrics['degree_centrality'] = nx.degree_centrality(G)
            metrics['betweenness_centrality'] = nx.betweenness_centrality(G)
            metrics['closeness_centrality'] = nx.closeness_centrality(G)
            
            # En merkezi kelimeler
            top_degree = sorted(metrics['degree_centrality'].items(), key=lambda x: x[1], reverse=True)[:10]
            top_betweenness = sorted(metrics['betweenness_centrality'].items(), key=lambda x: x[1], reverse=True)[:10]
            
            metrics['top_degree_words'] = top_degree
            metrics['top_betweenness_words'] = top_betweenness
        
        # Topluluk tespiti
        if G.number_of_nodes() > 2:
            try:
                communities = list(nx.community.greedy_modularity_communities(G))
                metrics['num_communities'] = len(communities)
                metrics['modularity'] = nx.community.modularity(G, communities)
                metrics['communities'] = [list(comm) for comm in communities]
            except:
                metrics['num_communities'] = 1
                metrics['modularity'] = 0
                metrics['communities'] = [list(G.nodes())]
        
        return metrics
    
    def visualize_network(self, G: nx.Graph, metrics: Dict, title: str = "Kelime Ä°liÅŸkileri Network'Ã¼"):
        """
        Network grafiÄŸini gÃ¶rselleÅŸtirir
        
        Args:
            G (nx.Graph): Network grafiÄŸi
            metrics (Dict): Network metrikleri
            title (str): Grafik baÅŸlÄ±ÄŸÄ±
        """
        plt.figure(figsize=(15, 12))
        
        # Layout hesapla
        pos = nx.spring_layout(G, k=3, iterations=50, seed=42)
        
        # Node'larÄ± Ã§iz
        node_sizes = [G.nodes[node]['size'] for node in G.nodes()]
        node_colors = [G.nodes[node]['weight'] for node in G.nodes()]
        
        nx.draw_networkx_nodes(G, pos, 
                              node_size=node_sizes,
                              node_color=node_colors,
                              cmap=plt.cm.viridis,
                              alpha=0.8)
        
        # Edge'leri Ã§iz
        edge_weights = [G[u][v]['width'] for u, v in G.edges()]
        nx.draw_networkx_edges(G, pos, 
                              width=edge_weights,
                              alpha=0.6,
                              edge_color='gray')
        
        # Etiketleri Ã§iz
        nx.draw_networkx_labels(G, pos, 
                               font_size=8,
                               font_weight='bold')
        
        plt.title(title, fontsize=16, fontweight='bold')
        plt.axis('off')
        
        # Metrikleri gÃ¶ster
        info_text = f"Node: {metrics['num_nodes']}, Edge: {metrics['num_edges']}, YoÄŸunluk: {metrics['density']:.3f}"
        if 'num_communities' in metrics:
            info_text += f", Topluluk: {metrics['num_communities']}"
        plt.figtext(0.5, 0.02, info_text, ha='center', fontsize=12)
        
        plt.tight_layout()
        plt.show()
    
    def print_network_summary(self, metrics: Dict):
        """
        Network analizi Ã¶zetini yazdÄ±rÄ±r
        
        Args:
            metrics (Dict): Network metrikleri
        """
        print("\n=== NETWORK ANALÄ°ZÄ° Ã–ZETÄ° ===")
        print(f"ğŸ“Š Temel Metrikler:")
        print(f"   Node SayÄ±sÄ±: {metrics['num_nodes']}")
        print(f"   Edge SayÄ±sÄ±: {metrics['num_edges']}")
        print(f"   YoÄŸunluk: {metrics['density']:.3f}")
        
        if 'num_communities' in metrics:
            print(f"   Topluluk SayÄ±sÄ±: {metrics['num_communities']}")
            print(f"   ModÃ¼lerlik: {metrics['modularity']:.3f}")
        
        if 'top_degree_words' in metrics:
            print(f"\nğŸ† En Merkezi Kelimeler (Derece):")
            for i, (word, centrality) in enumerate(metrics['top_degree_words'][:5], 1):
                print(f"   {i}. {word}: {centrality:.3f}")
        
        if 'top_betweenness_words' in metrics:
            print(f"\nğŸŒ En KÃ¶prÃ¼ Kelimeler (Betweenness):")
            for i, (word, centrality) in enumerate(metrics['top_betweenness_words'][:5], 1):
                print(f"   {i}. {word}: {centrality:.3f}")
        
        if 'communities' in metrics:
            print(f"\nğŸ‘¥ Kelime TopluluklarÄ±:")
            for i, community in enumerate(metrics['communities'][:3], 1):
                words = community[:5]  # Ä°lk 5 kelime
                print(f"   Topluluk {i}: {', '.join(words)}...")
    
    def analyze_networks(self, texts: List[str]) -> Dict:
        """
        Alias: analyze_network fonksiyonunu ana akÄ±ÅŸ ile uyumlu ÅŸekilde Ã§aÄŸÄ±rÄ±r
        """
        return self.analyze_network(texts)

    def analyze_network(self, texts: List[str]) -> Dict:
        """
        Metinler Ã¼zerinden network analizi yapar
        Args:
            texts (List[str]): TemizlenmiÅŸ metinler
        Returns:
            Dict: Network analiz sonuÃ§larÄ±
        """
        logger.info("Network analizi baÅŸlatÄ±lÄ±yor...")
        cooccurrences = self.extract_cooccurrences(texts)
        G = self.build_network(cooccurrences)
        metrics = self.calculate_network_metrics(G)
        results = {
            'cooccurrences': cooccurrences,
            'graph': G,
            'metrics': metrics
        }
        logger.info("Network analizi tamamlandÄ±")
        return results

# Test iÃ§in Ã¶rnek kullanÄ±m
if __name__ == "__main__":
    # Ã–rnek metinler
    sample_texts = [
        "ekonomi bÃ¼yÃ¼me hedef revize edildi bakan aÃ§Ä±klama",
        "bakan ekonomi bÃ¼yÃ¼me aÃ§Ä±klama yaptÄ±",
        "bÃ¼yÃ¼me hedef ekonomi 2024",
        "siyaset cumhurbaÅŸkanÄ± toplantÄ± karar",
        "cumhurbaÅŸkanÄ± siyaset karar aldÄ±",
        "spor futbol maÃ§ sonuÃ§ galibiyet",
        "futbol spor maÃ§ sonuÃ§",
        "hava durumu sÄ±caklÄ±k yaÄŸÄ±ÅŸ",
        "sÄ±caklÄ±k hava durumu artÄ±ÅŸ"
    ]
    
    # Network analizi
    analyzer = NetworkAnalyzer(min_cooccurrence=1, max_nodes=20)
    results = analyzer.analyze_network(sample_texts) 