"""
GeliÅŸmiÅŸ Analiz ModÃ¼lÃ¼

Bu modÃ¼l, haber verilerinde anlamlÄ± ve iÅŸlevsel analizler yapar:
- Zaman serisi ve trend analizi
- Otomatik kategori etiketleme
- Anomali ve olay tespiti
- GÃ¼ndem haritasÄ±
- Otomatik alarm sistemi
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict, Counter
import re
from typing import List, Dict, Tuple
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class AdvancedAnalytics:
    """GeliÅŸmiÅŸ haber analizi sÄ±nÄ±fÄ±"""
    
    def __init__(self):
        # Kategori anahtar kelimeleri
        self.category_keywords = {
            'gÃ¼ndem': ['cumhurbaÅŸkanÄ±', 'bakan', 'meclis', 'hÃ¼kÃ¼met', 'siyaset'],
            'ekonomi': ['ekonomi', 'borsa', 'dolar', 'enflasyon', 'faiz', 'bÃ¼tÃ§e'],
            'spor': ['futbol', 'basketbol', 'maÃ§', 'lig', 'ÅŸampiyon', 'spor'],
            'dÃ¼nya': ['abd', 'rusya', 'avrupa', 'bm', 'nato', 'dÃ¼nya'],
            'teknoloji': ['teknoloji', 'yapay zeka', 'internet', 'dijital', 'yazÄ±lÄ±m'],
            'saÄŸlÄ±k': ['saÄŸlÄ±k', 'hastane', 'doktor', 'tedavi', 'ilaÃ§', 'virÃ¼s'],
            'eÄŸitim': ['eÄŸitim', 'okul', 'Ã¼niversite', 'Ã¶ÄŸrenci', 'sÄ±nav'],
            'Ã§evre': ['Ã§evre', 'iklim', 'doÄŸa', 'kirlilik', 'yeÅŸil']
        }
        
    def extract_time_features(self, news_items: List[Dict]) -> pd.DataFrame:
        """
        Haber verilerinden zaman Ã¶zelliklerini Ã§Ä±karÄ±r
        """
        df = pd.DataFrame(news_items)
        df['published'] = pd.to_datetime(df['published'])
        df['date'] = df['published'].dt.date
        df['hour'] = df['published'].dt.hour
        df['day_of_week'] = df['published'].dt.day_name()
        df['is_weekend'] = df['published'].dt.weekday >= 5
        return df
    
    def analyze_trends(self, news_items: List[Dict], keyword: str = None) -> Dict:
        """
        Zaman serisi trend analizi yapar
        """
        logger.info("Trend analizi baÅŸlatÄ±lÄ±yor...")
        
        df = self.extract_time_features(news_items)
        
        # GÃ¼nlÃ¼k haber yoÄŸunluÄŸu
        daily_counts = df.groupby('date').size().reset_index(name='count')
        daily_counts['date'] = pd.to_datetime(daily_counts['date'])
        
        # Trend hesaplama (7 gÃ¼nlÃ¼k hareketli ortalama)
        daily_counts['trend'] = daily_counts['count'].rolling(window=7, min_periods=1).mean()
        
        # Belirli kelime iÃ§in trend
        if keyword:
            keyword_counts = df[df['title'].str.contains(keyword, case=False, na=False) | 
                               df['summary'].str.contains(keyword, case=False, na=False)]
            keyword_daily = keyword_counts.groupby('date').size().reset_index(name='keyword_count')
            keyword_daily['date'] = pd.to_datetime(keyword_daily['date'])
            keyword_daily['keyword_trend'] = keyword_daily['keyword_count'].rolling(window=3, min_periods=1).mean()
        else:
            keyword_daily = None
        
        # Anomali tespiti (haber yoÄŸunluÄŸunda ani artÄ±ÅŸlar)
        if len(daily_counts) > 0:
            anomaly_detector = IsolationForest(contamination=0.1, random_state=42)
            daily_counts['anomaly'] = anomaly_detector.fit_predict(daily_counts[['count']].values)
        else:
            daily_counts['anomaly'] = []
        
        # En yoÄŸun gÃ¼nler
        peak_days = daily_counts.nlargest(5, 'count')[['date', 'count']]
        
        results = {
            'daily_counts': daily_counts,
            'keyword_trends': keyword_daily,
            'peak_days': peak_days,
            'total_news': len(df),
            'date_range': (df['published'].min(), df['published'].max())
        }
        
        logger.info("Trend analizi tamamlandÄ±")
        return results
    
    def categorize_news(self, news_items: List[Dict]) -> Dict:
        """
        Haberleri otomatik olarak kategorilere ayÄ±rÄ±r
        """
        logger.info("Haber kategorizasyonu baÅŸlatÄ±lÄ±yor...")
        
        categorized_news = defaultdict(list)
        category_scores = defaultdict(int)
        
        for item in news_items:
            text = f"{item['title']} {item['summary']}".lower()
            best_category = 'diÄŸer'
            best_score = 0
            
            for category, keywords in self.category_keywords.items():
                score = sum(1 for keyword in keywords if keyword in text)
                if score > best_score:
                    best_score = score
                    best_category = category
            
            categorized_news[best_category].append(item)
            category_scores[best_category] += 1
        
        # Kategori daÄŸÄ±lÄ±mÄ±
        category_distribution = dict(category_scores)
        
        # En popÃ¼ler kategoriler
        top_categories = sorted(category_scores.items(), key=lambda x: x[1], reverse=True)
        
        results = {
            'categorized_news': dict(categorized_news),
            'category_distribution': category_distribution,
            'top_categories': top_categories
        }
        
        logger.info("Haber kategorizasyonu tamamlandÄ±")
        return results
    
    def detect_events(self, news_items: List[Dict]) -> Dict:
        """
        Ã–nemli olaylarÄ± ve anomalileri tespit eder
        """
        logger.info("Olay tespiti baÅŸlatÄ±lÄ±yor...")
        
        df = self.extract_time_features(news_items)
        
        # Saatlik haber yoÄŸunluÄŸu
        hourly_counts = df.groupby('hour').size()
        
        # Anormal saatler (ortalama + 2 standart sapma)
        mean_hourly = hourly_counts.mean()
        std_hourly = hourly_counts.std()
        anomaly_hours = hourly_counts[hourly_counts > mean_hourly + 2*std_hourly]
        
        # GÃ¼nlÃ¼k yoÄŸunluk anomalileri
        daily_counts = df.groupby('date').size()
        mean_daily = daily_counts.mean()
        std_daily = daily_counts.std()
        anomaly_days = daily_counts[daily_counts > mean_daily + 2*std_daily]
        
        # Acil durum kelimeleri
        emergency_keywords = ['deprem', 'yangÄ±n', 'kazasÄ±', 'Ã¶lÃ¼m', 'kriz', 'acil', 'patlama']
        emergency_news = []
        
        for item in news_items:
            text = f"{item['title']} {item['summary']}".lower()
            if any(keyword in text for keyword in emergency_keywords):
                emergency_news.append(item)
        
        results = {
            'anomaly_hours': anomaly_hours.to_dict(),
            'anomaly_days': anomaly_days.to_dict(),
            'emergency_news': emergency_news,
            'total_emergency': len(emergency_news)
        }
        
        logger.info("Olay tespiti tamamlandÄ±")
        return results
    
    def analyze_source_comparison(self, news_items: List[Dict]) -> Dict:
        """
        FarklÄ± haber kaynaklarÄ±nÄ± karÅŸÄ±laÅŸtÄ±rÄ±r
        """
        logger.info("Kaynak karÅŸÄ±laÅŸtÄ±rmasÄ± baÅŸlatÄ±lÄ±yor...")
        
        df = pd.DataFrame(news_items)
        
        # Kaynak bazÄ±nda kategori daÄŸÄ±lÄ±mÄ±
        source_category_analysis = {}
        
        for source in df['source'].unique():
            source_news = df[df['source'] == source]
            categorized = self.categorize_news(source_news.to_dict('records'))
            source_category_analysis[source] = categorized['category_distribution']
        
        # Kaynak bazÄ±nda haber yoÄŸunluÄŸu
        source_counts = df['source'].value_counts()
        
        # Kaynak bazÄ±nda ortalama haber uzunluÄŸu
        df['title_length'] = df['title'].str.len()
        df['summary_length'] = df['summary'].str.len()
        source_avg_lengths = df.groupby('source')[['title_length', 'summary_length']].mean()
        
        results = {
            'source_category_analysis': source_category_analysis,
            'source_counts': source_counts.to_dict(),
            'source_avg_lengths': source_avg_lengths.to_dict()
        }
        
        logger.info("Kaynak karÅŸÄ±laÅŸtÄ±rmasÄ± tamamlandÄ±")
        return results
    
    def generate_alerts(self, news_items: List[Dict], alert_thresholds: Dict = None) -> List[Dict]:
        """
        Otomatik alarm ve bildirimler Ã¼retir
        """
        logger.info("Alarm sistemi baÅŸlatÄ±lÄ±yor...")
        
        if alert_thresholds is None:
            alert_thresholds = {
                'emergency_keywords': 5,  # Acil durum kelimesi geÃ§en haber sayÄ±sÄ±
                'daily_news_spike': 50,   # GÃ¼nlÃ¼k haber sayÄ±sÄ± artÄ±ÅŸÄ±
                'category_spike': 20      # Kategori bazÄ±nda artÄ±ÅŸ
            }
        
        alerts = []
        
        # Acil durum alarmlarÄ±
        emergency_keywords = ['deprem', 'yangÄ±n', 'kazasÄ±', 'Ã¶lÃ¼m', 'kriz', 'acil']
        emergency_count = 0
        
        for item in news_items:
            text = f"{item['title']} {item['summary']}".lower()
            if any(keyword in text for keyword in emergency_keywords):
                emergency_count += 1
        
        if emergency_count >= alert_thresholds['emergency_keywords']:
            alerts.append({
                'type': 'emergency',
                'message': f'âš ï¸ Acil durum haberlerinde artÄ±ÅŸ: {emergency_count} haber',
                'severity': 'high',
                'timestamp': datetime.now()
            })
        
        # GÃ¼nlÃ¼k yoÄŸunluk alarmlarÄ±
        df = self.extract_time_features(news_items)
        today_count = len(df[df['date'] == datetime.now().date()])
        
        if today_count >= alert_thresholds['daily_news_spike']:
            alerts.append({
                'type': 'volume',
                'message': f'ğŸ“ˆ GÃ¼nlÃ¼k haber yoÄŸunluÄŸu yÃ¼ksek: {today_count} haber',
                'severity': 'medium',
                'timestamp': datetime.now()
            })
        
        # Kategori alarmlarÄ±
        categorized = self.categorize_news(news_items)
        for category, count in categorized['top_categories']:
            if count >= alert_thresholds['category_spike']:
                alerts.append({
                    'type': 'category',
                    'message': f'ğŸ·ï¸ {category} kategorisinde yoÄŸunluk: {count} haber',
                    'severity': 'low',
                    'timestamp': datetime.now()
                })
        
        logger.info(f"{len(alerts)} alarm Ã¼retildi")
        return alerts
    
    def create_agenda_map(self, news_items: List[Dict]) -> Dict:
        """
        GÃ¼ndem haritasÄ± oluÅŸturur
        """
        logger.info("GÃ¼ndem haritasÄ± oluÅŸturuluyor...")
        
        df = self.extract_time_features(news_items)
        
        # GÃ¼nlÃ¼k konu daÄŸÄ±lÄ±mÄ±
        daily_topics = {}
        
        for date in df['date'].unique():
            daily_news = df[df['date'] == date]
            # GÃ¼nlÃ¼k en popÃ¼ler kelimeler
            all_text = ' '.join(daily_news['title'].tolist() + daily_news['summary'].tolist())
            words = re.findall(r'\b\w+\b', all_text.lower())
            word_counts = Counter(words)
            daily_topics[date] = word_counts.most_common(5)
        
        # HaftalÄ±k trend analizi
        df['week'] = df['published'].dt.isocalendar().week
        weekly_trends = df.groupby('week').size()
        
        # Konu deÄŸiÅŸim hÄ±zÄ±
        topic_velocity = {}
        dates = sorted(daily_topics.keys())
        
        for i in range(1, len(dates)):
            prev_topics = set(word for word, _ in daily_topics[dates[i-1]])
            curr_topics = set(word for word, _ in daily_topics[dates[i]])
            
            new_topics = curr_topics - prev_topics
            dropped_topics = prev_topics - curr_topics
            
            topic_velocity[dates[i]] = {
                'new': list(new_topics),
                'dropped': list(dropped_topics),
                'stable': list(prev_topics & curr_topics)
            }
        
        results = {
            'daily_topics': daily_topics,
            'weekly_trends': weekly_trends.to_dict(),
            'topic_velocity': topic_velocity
        }
        
        logger.info("GÃ¼ndem haritasÄ± oluÅŸturuldu")
        return results

# Test iÃ§in Ã¶rnek kullanÄ±m
if __name__ == "__main__":
    # Ã–rnek haber verileri
    sample_news = [
        {
            'title': 'Deprem sonrasÄ± yardÄ±m Ã§alÄ±ÅŸmalarÄ±',
            'summary': 'Ä°stanbul\'da meydana gelen deprem sonrasÄ± yardÄ±m Ã§alÄ±ÅŸmalarÄ± baÅŸladÄ±.',
            'published': '2024-01-15T10:00:00',
            'source': 'https://example.com'
        },
        {
            'title': 'Ekonomi bÃ¼yÃ¼me verileri aÃ§Ä±klandÄ±',
            'summary': 'TÃ¼rkiye ekonomisi yÃ¼zde 3 bÃ¼yÃ¼dÃ¼.',
            'published': '2024-01-15T11:00:00',
            'source': 'https://example.com'
        }
    ]
    
    analyzer = AdvancedAnalytics()
    
    # Trend analizi
    trends = analyzer.analyze_trends(sample_news, keyword='deprem')
    print("Trend analizi:", trends)
    
    # Kategorizasyon
    categories = analyzer.categorize_news(sample_news)
    print("Kategoriler:", categories)
    
    # Olay tespiti
    events = analyzer.detect_events(sample_news)
    print("Olaylar:", events)
    
    # Alarmlar
    alerts = analyzer.generate_alerts(sample_news)
    print("Alarmlar:", alerts) 